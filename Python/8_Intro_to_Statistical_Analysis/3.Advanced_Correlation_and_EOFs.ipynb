{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "860842b2",
   "metadata": {},
   "source": [
    "<img src=\"images/Project_logos.png\" width=\"500\" height=\"300\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d5c9bd",
   "metadata": {},
   "source": [
    "## Aims\n",
    "\n",
    "This course will teach you some advanced techniques to investigate and handle correlations in your data.\n",
    "\n",
    "\n",
    "Prior knowledge of Python, NumPy, Pandas, Iris, and Matplolib are assumed for this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd8cba",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Autocorrelation](#autocorrelation)\n",
    "* [Examples using NumPy](#examples_numpy)\n",
    "* [Examples using Pandas](#examples_pandas)\n",
    "* [Empirical Orthogonal Functions](#eofs)\n",
    "* [Exercise 1](#exercise1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b39b9d",
   "metadata": {},
   "source": [
    "## Autocorrelation<a class=\"anchor\" id=\"autocorrelation\"></a>\n",
    "\n",
    "Environmental data are usually collected as snapshots in time rather than as continuous variables, for example measurements may be taken hourly, daily, monthly, yearly. The closer in time the measurements are taken, the more similar the measurements will be: an hourly datapoint will be more similar to the previous hourly datapoint than a yearly datapoint will be to the previous yearly datapoint.  This means that data can be **autocorrelated** across a given time lag. \n",
    "\n",
    "Autocorrelation is usually calculated in terms of different lag times and is useful to identify the dependence, persistence or memory within the data. Temporal correlation can also be termed **serial correlation** or **lagged correlation** and is essentially the correlation between a datapoint and a datapoint from the previous time steps of data collection.\n",
    "\n",
    "- **Positive Autocorrelation**: an increase in one datapoint value predicts an increase in another datapoint value at a different timestep\n",
    "- **Negative Autocorrelation**: an increase in one datapoint value predicts a decrease in another datapoint value at a different timestep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d7d8d4",
   "metadata": {},
   "source": [
    "## Examples using the NumPy library<a class=\"anchor\" id=\"examples_numpy\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aad3358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load in the data\n",
    "data = genfromtxt('data/heathrow_weather_station_data.csv', delimiter=',')\n",
    "\n",
    "# select the maximum temperature data\n",
    "tmax = data[1:,2]\n",
    "\n",
    "# calculate the autocorrelation with a time lag of 1 month\n",
    "lag1_autocorrelation = np.corrcoef(tmax[1:-1], tmax[2:])[0,1]\n",
    "print(f'Lag 1 month autocorrelation = {lag1_autocorrelation}\\n')\n",
    "\n",
    "# plot the autocovariances for 100 lags\n",
    "# Note that here you can see the seasonal cycle, and that the relationship\n",
    "# between the monthly values and the values for previous years diminishes\n",
    "# as the lag increases.\n",
    "fig = plt.figure()\n",
    "plt.xlabel(\"Lag (months)\")\n",
    "plt.ylabel(\"Autocorrelation\")\n",
    "plt.acorr(tmax, maxlags = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9967dc57",
   "metadata": {},
   "source": [
    "<font color='red'>**NOTE**</font>:  The statistical autocorrelation is normalized to be between -1 and 1, whereas the signal processing definition is not, and any unstandardised autocorrelations, such as that plotted above, are more correctly termed autocovariances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3993642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot the autocorrelations using this method, first normalize the data:\n",
    "mean = sum(tmax) / len(tmax) \n",
    "var = sum([(x - mean)**2 for x in tmax]) / len(tmax) \n",
    "# Normalized data\n",
    "tmax = [x - mean for x in tmax]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.xlabel(\"Lag (months)\")\n",
    "plt.ylabel(\"Autocorrelation\")\n",
    "plt.acorr(tmax, maxlags = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3527d3",
   "metadata": {},
   "source": [
    "## Examples using the Pandas library<a class=\"anchor\" id=\"examples_pandas\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d12e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.graphics import tsaplots\n",
    "\n",
    "# Load in the data\n",
    "df_data = pd.read_csv('data/heathrow_weather_station_data.csv')\n",
    "\n",
    "# print the monthly data\n",
    "print(df_data)\n",
    "\n",
    "# select the maximum temperature data\n",
    "df_tmax = df_data['Max_temperature']\n",
    "\n",
    "# calculate the autocorrelation with a time lag of 1 month\n",
    "lag1_autocorrelation = df_tmax.autocorr()\n",
    "print(f'\\n\\nLag 1 month autocorrelation = {lag1_autocorrelation}\\n')\n",
    "\n",
    "# plot the relationship between the monthly maximum temperature\n",
    "# and the maximum temperature the following month\n",
    "pd.plotting.lag_plot(df_tmax, lag=1)\n",
    "\n",
    "# plot the autocorrelations for all lags\n",
    "fig = plt.figure()\n",
    "plt.xlabel(\"Lag (months)\")\n",
    "plt.ylabel(\"Autocorrelation\")\n",
    "autocorrelation_plot(df_tmax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a2485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics import tsaplots\n",
    "\n",
    "# another version of plotting the autocorrelations for 100 lags\n",
    "# in this plot the shading shows the 95% confidence interval\n",
    "# meaning that any datapoints within the shaded area have no \n",
    "# significant correlation with the most recent datapoint.\n",
    "fig = tsaplots.plot_acf(df_tmax, lags=100)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46880396",
   "metadata": {},
   "source": [
    "## Empirical Orthogonal Functions<a class=\"anchor\" id=\"eofs\"></a>\n",
    "Data can also be correlated in space and a common method for examining patterns in data that varies across space and time is to create Empirical Orthogonal Function (EOFs) that decompose the data into a spatial component and a temporal component. A complete description of EOF analysis is beyond the scope of this course and it is recommended that further reading be conducted. Here only the methods for conducting an EOF analysis in Python are demonstrated.\n",
    "\n",
    "Here we provide an example of using sea surface temperature (SST) data to identify leafing modes of climate variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bdc88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris\n",
    "import numpy as np\n",
    "from eofs.iris import Eof\n",
    "import pandas as pd\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import iris.plot as iplt\n",
    "\n",
    "def detrend_cube(cube, order, dim=0):\n",
    "    # returns the array of regression coeficients, the fitted y values (same size as cube) and the\n",
    "    # cube of the detrended data\n",
    "    coord = cube.coords(contains_dimension=dim, dim_coords=True)[0]\n",
    "    cs = cube.shape\n",
    "    A2 = cube.data.reshape(cs[dim], -1)\n",
    "    reg = np.polyfit(coord.points, A2, order)\n",
    "    yfit = [np.polyval(reg[:, x], coord.points) for x in range(reg.shape[1])]\n",
    "    yfitarr = np.array(yfit).T.reshape(cs)\n",
    "    ncube = cube.copy()\n",
    "\n",
    "    ncube.data = ncube.data-yfitarr\n",
    "    # retain original mask if available\n",
    "    if isinstance(cube.data, np.ma.MaskedArray):\n",
    "        ncube.data = np.ma.MaskedArray(ncube.data, mask=cube.data.mask)\n",
    "    return reg, yfitarr, ncube\n",
    "\n",
    "\n",
    "# Load in the SST data for December-March\n",
    "cube = iris.load_cube('data/OBS-ERA5_5deg_sst_jfm_1979_2018_low_res_5deg.nc')\n",
    "\n",
    "# Linearly detrend the data\n",
    "_, _, cube = detrend_cube(cube, 1, dim=0)\n",
    "\n",
    "solver = Eof(cube, weights='coslat')\n",
    "eofs = solver.eofsAsCovariance(neofs=4)\n",
    "eigenvalues = solver.eigenvalues()\n",
    "pcs = solver.pcs(npcs=4, pcscaling=1)\n",
    "    \n",
    "# Plot the spatial pattern of the leading 4 EOFs expressed as covariance.\n",
    "fig = plt.figure()\n",
    "\n",
    "for e in [0,1,2,3]:\n",
    "    ax = fig.add_subplot(2,2,e+1, projection=ccrs.PlateCarree())\n",
    "    iplt.contourf(eofs[e], cmap=plt.cm.RdBu_r)\n",
    "    ax.set_title(f'EOF{e+1} ({int((eigenvalues.data[e]/eigenvalues.data.sum())*100)}% of variance explained)', fontsize=10)\n",
    "    ax.coastlines()\n",
    "    ax.set_global()\n",
    "plt.tight_layout()\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13021cc1",
   "metadata": {},
   "source": [
    "Since the first EOF looks like the El Nino Southern Oscillation (ENSO), we can plot this as a timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d144f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the SST data for December-March\n",
    "cube = iris.load_cube('data/OBS-ERA5_5deg_sst_jfm_1979_2018_low_res_5deg.nc')\n",
    "\n",
    "# Linearly detrend the data\n",
    "_, _, cube = detrend_cube(cube, 1, dim=0)\n",
    "\n",
    "# Subset the ENSO region (5S-5N and 170-120W)\n",
    "subset_cube = cube.intersection(longitude=(-170, -120), latitude=(-5, 5))\n",
    "\n",
    "# Calculate the EOFs for the ENSO region\n",
    "solver = Eof(subset_cube, weights='coslat')\n",
    "eofs = solver.eofsAsCovariance(neofs=1)\n",
    "eigenvalues = solver.eigenvalues()\n",
    "pcs = solver.pcs(npcs=1, pcscaling=1)\n",
    "\n",
    "# Save the timeseries to a Pandas DataFrame\n",
    "pc_df = pd.DataFrame(pcs.data, columns=['EOF1'])\n",
    "pc_df['Year'] = cube.coord('season_year').points\n",
    "\n",
    "# Plot the timeseries\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(pc_df['Year'], pc_df[f'EOF1'], color='k', linewidth=0.5)\n",
    "ax.set_title(f'EOF1 (ENSO)', fontsize=16)\n",
    "pc_df[f'POS_EOF1'] = 0\n",
    "pc_df[f'NEG_EOF1'] = 0\n",
    "pos_threshold = 0.5\n",
    "neg_threshold = -0.5\n",
    "pc_df.loc[pc_df[f'EOF1'] >pos_threshold, f'POS_EOF1'] = 1\n",
    "pc_df.loc[pc_df[f'EOF1'] <neg_threshold, f'NEG_EOF1'] = 1\n",
    "num_pos = pc_df[f'POS_EOF1'].sum()\n",
    "num_neg = pc_df[f'NEG_EOF1'].sum()\n",
    "plt.fill_between(pc_df['Year'], pos_threshold, pc_df[f'EOF1'], color='red',\n",
    "                 where=pc_df[f'EOF1']>=pos_threshold, interpolate=True)\n",
    "plt.fill_between(pc_df['Year'], neg_threshold, pc_df[f'EOF1'], color='blue',\n",
    "                 where=pc_df[f'EOF1']<=neg_threshold, interpolate=True)\n",
    "plt.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "plt.text(1978, -3.0, f'{num_pos} El Nino events', horizontalalignment='left',\n",
    "         verticalalignment='center')\n",
    "plt.text(1978, -3.3, f'{num_neg} La Nina events', horizontalalignment='left',\n",
    "         verticalalignment='center')\n",
    "plt.tight_layout()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d0530d",
   "metadata": {},
   "source": [
    "## Exercise 1<a class=\"anchor\" id=\"exercise_1\"></a>\n",
    "\n",
    "Using any Python methods, calculate and plot a map and the timeseries for the Summer North Atlantic Oscillation (SNAO) defined as the first EOF of June-August mean sea level pressure for region 40N–70N, 90W–30E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ea754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris\n",
    "\n",
    "cube = iris.load_cube('data/OBS-ERA5_5deg_mslp_jja_1979_2018_low_res_5deg.nc')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8daa2be",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**Solution**\n",
    "\n",
    "<font color='red'>**NOTE**</font>: Your methods can include any Python library\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbc7836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris\n",
    "\n",
    "cube = iris.load_cube('data/OBS-ERA5_5deg_mslp_jja_1979_2018_low_res_5deg.nc')\n",
    "    \n",
    "# Subset the SNAO region (40N–70N, 90W–30E)\n",
    "subset_cube = cube.intersection(longitude=(-90, 30), latitude=(40, 70))\n",
    "print(subset_cube)\n",
    "\n",
    "# Calculate the EOFs for the SNAO region\n",
    "solver = Eof(subset_cube, weights='coslat')\n",
    "eofs = solver.eofsAsCovariance(neofs=1)\n",
    "eigenvalues = solver.eigenvalues()\n",
    "pcs = solver.pcs(npcs=1, pcscaling=1)\n",
    "\n",
    "# Plot the spatial pattern of the leading EOF expressed as covariance.\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(111, projection=ccrs.PlateCarree())\n",
    "iplt.contourf(eofs[0], cmap=plt.cm.RdBu_r)\n",
    "ax.set_title(f'EOF1 ({int((eigenvalues.data[e]/eigenvalues.data.sum())*100)}% of variance explained)', fontsize=10)\n",
    "ax.coastlines()\n",
    "ax.set_global()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the timeseries to a Pandas DataFrame\n",
    "pc_df = pd.DataFrame(pcs.data, columns=['EOF1'])\n",
    "pc_df['Year'] = subset_cube.coord('season_year').points\n",
    "\n",
    "# Plot the timeseries\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(pc_df['Year'], pc_df[f'EOF1'], color='k', linewidth=0.5)\n",
    "ax.set_title(f'EOF1 (SNAO)', fontsize=16)\n",
    "pc_df[f'POS_EOF1'] = 0\n",
    "pc_df[f'NEG_EOF1'] = 0\n",
    "pos_threshold = 0.5\n",
    "neg_threshold = -0.5\n",
    "pc_df.loc[pc_df[f'EOF1'] >pos_threshold, f'POS_EOF1'] = 1\n",
    "pc_df.loc[pc_df[f'EOF1'] <neg_threshold, f'NEG_EOF1'] = 1\n",
    "num_pos = pc_df[f'POS_EOF1'].sum()\n",
    "num_neg = pc_df[f'NEG_EOF1'].sum()\n",
    "plt.fill_between(pc_df['Year'], pos_threshold, pc_df[f'EOF1'], color='red',\n",
    "                 where=pc_df[f'EOF1']>=pos_threshold, interpolate=True)\n",
    "plt.fill_between(pc_df['Year'], neg_threshold, pc_df[f'EOF1'], color='blue',\n",
    "                 where=pc_df[f'EOF1']<=neg_threshold, interpolate=True)\n",
    "plt.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "plt.text(1978, -3.0, f'{num_pos} Positive SNAO events', horizontalalignment='left',\n",
    "         verticalalignment='center')\n",
    "plt.text(1978, -3.3, f'{num_neg} Negative SNAO events', horizontalalignment='left',\n",
    "         verticalalignment='center')\n",
    "plt.tight_layout()\n",
    "plt.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Jaspy",
   "language": "python",
   "name": "jaspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
