{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b46ce86b",
   "metadata": {},
   "source": [
    "<img src=\"images/Project_logos.png\" width=\"500\" height=\"300\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d8d6e",
   "metadata": {},
   "source": [
    "## Descriptive Statistics\n",
    "\n",
    "Descriptive statistics describe and summarise datasets and can be:\n",
    "\n",
    "- quantitative (statistical analysis of the data)\n",
    "- qualitative (visualisations of the data)\n",
    "\n",
    "Prior knowledge of Python, NumPy, Pandas, Iris, and Matplolib are assumed for this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d5c9bd",
   "metadata": {},
   "source": [
    "## Aims\n",
    "\n",
    "This course will teach you about commonly used statistical terminology and Python libraries for the following measures of descriptive statistics:\n",
    "\n",
    "  - central tendency, the 'average' values for the data\n",
    "  - variability, the spread of the data\n",
    "  - data visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd8cba",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Glossay of Statistical Terminology](#glossary)\n",
    "* [Central Tendency](#central_tendency)\n",
    "* [Exercise 1](#exercise_1)\n",
    "* [Variability](#variability)\n",
    "* [Exercise 2](#exercise_2)\n",
    "* [Visualisation](#visualisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396c849",
   "metadata": {},
   "source": [
    "## Glossary of Statistical Terminology<a class=\"anchor\" id=\"glossary\"></a>\n",
    "\n",
    "- **Population:** the set of all elements in the dataset\n",
    "- **Sample:** a subset of a population\n",
    "- **Outlier:** a data point that is significantly different from the rest of the sample or population\n",
    "- **Mean:** the arithmetic average of all the elements in the dataset (the sum of all the elements divided by the number of elements)\n",
    "- **Median** the middle element in a sorted (ascending or descending) dataset\n",
    "- **Mode** the most frequently occuring value in the dataset\n",
    "- **Weighted Mean:** the arithmetic average of all the elements in the dataset multiplied by a weighting factor for each element (the sum of all the elements multiplied by their respective weights, divided by the sum of all the weights)\n",
    "- **Variance:** a measure showing how far the dataset elements are from the mean\n",
    "- **Standard Deviation:** a measure showing how far the dataset elements are from the mean, in the same units as the dataset elements\n",
    "- **Skewness:** a measure of the asymmetry in the dataset\n",
    "- **Kurtosis:** a measure of tailedness; how much of the data lies in the tails of the distribution, how often outliers occur.\n",
    "- **Percentile:** the value in the dataset below which a certain percentage of the data falls\n",
    "- **Interquartile Range:** the difference between the 25th and 75th percentile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b39b9d",
   "metadata": {},
   "source": [
    "## Central Tendency<a class=\"anchor\" id=\"central_tendency\"></a>\n",
    "\n",
    "Measures of central tendency provide estimates of the most typical value in a dataset and there are different definitions. Here we cover the most common.\n",
    "\n",
    "The sample **mean** is the arithmetic average of all the elements in the dataset (the sum of all the elements divided by the number of elements).\n",
    "\n",
    "The sample **median** is the middle element in a sorted (ascending or descending) dataset. The mean is heavily affected by outliers, but the median only depends on outliers either slightly or not at all.\n",
    "\n",
    "The sample **mode** is the most frequently occuring value in the dataset. Where more than one value occurs in the dataset with the same frequency, the result will be multimodal and all modes will be returned. \n",
    "\n",
    "The sample **weighted mean** is the arithmetic average of all the elements in the dataset multiplied by a weighting factor for each element (the sum of all the elements multiplied by their respective weights, divided by the sum of all the weights). The weights could represent the frequency of the elements, the area covered by each data element, or a user-defined weighting according to importance of different factors in the data (e.g. the average uncertainty for temperature measurements could be based on the number of observations at a given location, the length of time of the total record, and the age of the observing instrumentation and each could be assigned a different weight).\n",
    "\n",
    "The measures of central tendency can all be calculated using many Python libraries, including the built-in library `statistics`, and the libraries `NumPy`, `Pandas` and `Iris`, depending on the type of data being analysed. Note that for calculating the median or the mode for some datatypes, a dedicated statistical library will need to be used, such as `SciPy`.\n",
    "\n",
    "Note that you will need to be careful which method is choosen if there are NaNs in the dataset because, for some libraries, NaN will be returned if the sample contains NaNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6277c546",
   "metadata": {},
   "source": [
    "### Examples using the statistics library ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b9712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "'''Mean'''\n",
    "sample = [2, 5, 4, 23, 4, 5, 15, 1]\n",
    "mean = statistics.mean(sample)\n",
    "print(f'Mean = {mean}')\n",
    "\n",
    "'''Median'''\n",
    "sample = [2, 5, 4, 23, 4, 5, 15, 1]\n",
    "median = statistics.median(sample)\n",
    "print(f'Median = {median}')\n",
    "\n",
    "'''Mode'''\n",
    "sample = [2, 5, 4, 23, 4, 5, 15, 1]\n",
    "multimode = statistics.multimode(sample)\n",
    "print(f'Mode = {multimode}')\n",
    "mode = statistics.mode(sample)\n",
    "print(f'Mode = {mode}. Note that here the multimodes have been rounded up')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d7d8d4",
   "metadata": {},
   "source": [
    "### Examples using the NumPy library ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aad3358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''Mean'''\n",
    "sample = [2, 5, 4, 23, 4, 5, 15, 1]\n",
    "mean = np.mean(sample)\n",
    "print(f'Mean = {mean}')\n",
    "\n",
    "# if the sample is a NumPy array you can also use:\n",
    "sample = np.array([2, 5, 4, 23, 4, 5, 15, 1])\n",
    "mean = sample.mean()\n",
    "print(f'Mean using arrays = {mean}')\n",
    "\n",
    "# To ignore NaNs use np.nanmean()\n",
    "\n",
    "'''Median'''\n",
    "sample = [2, 5, 4, 23, 4, 5, 15, 1]\n",
    "median = np.median(sample)\n",
    "print(f'Median = {median}')\n",
    "\n",
    "'''Mode'''\n",
    "import scipy.stats\n",
    "sample = np.array([2, 5, 4, 23, 4, 5, 15, 1])\n",
    "mode = scipy.stats.mode(sample)\n",
    "print(f'Mode = {mode}. Note that here only the smallest value for the mode is returned')\n",
    "\n",
    "\n",
    "'''Weighted Mean'''\n",
    "sample = np.array([2, 5, 4, 23, 4, 5, 15, 1])\n",
    "weights = np.array([10, 20, 50, 1, 4, 5, 8, 2])\n",
    "mean = np.average(sample, weights=weights)\n",
    "print(f'Weighted Mean = {mean}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3527d3",
   "metadata": {},
   "source": [
    "### Examples using the Pandas library ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d12e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "'''Mean'''\n",
    "sample = [2, 5, 4, 23, 4, 5, 15, 1]\n",
    "sample_df = pd.DataFrame(sample)\n",
    "mean = sample_df.mean().values\n",
    "print(f'Mean = {mean}')\n",
    "\n",
    "\n",
    "'''Median'''\n",
    "sample = [2, 5, 4, 23, 4, 5, 15, 1]\n",
    "sample_df = pd.DataFrame(sample)\n",
    "median = sample_df.median().values\n",
    "print(f'Median = {median}')\n",
    "\n",
    "\n",
    "'''Mode'''\n",
    "sample = [2, 5, 4, 23, 4, 5, 15, 1]\n",
    "sample_df = pd.DataFrame(sample)\n",
    "mode = sample_df.mode().values\n",
    "print(f'Mode = {mode[0]} and {mode[1]}. Note that two values are returned because 4 and 5 occur with the same frequency')\n",
    "\n",
    "\n",
    "'''Weighted Mean'''\n",
    "sample_df = pd.DataFrame({'Value':[2, 5, 4, 23, 4, 5, 15, 1],\n",
    "                          'Weight':[10, 20, 50, 1, 4, 5, 8, 2]})\n",
    "mean = (sample_df.Value * sample_df.Weight).sum() / sample_df.Weight.sum()\n",
    "print(f'Weighted Mean = {mean}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c50886",
   "metadata": {},
   "source": [
    "### Examples using the Iris library ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad2e1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris.cube\n",
    "\n",
    "'''Mean'''\n",
    "sample = [2, 5, 4, 23, 4, 5, 15, 1]\n",
    "cube = iris.cube.Cube(np.zeros((8), np.int8))\n",
    "cube.data = sample\n",
    "mean = cube.data.mean()\n",
    "print(f'Mean = {mean}')\n",
    "\n",
    "\n",
    "'''Median'''\n",
    "latitude = iris.coords.DimCoord(np.arange(-90, 90,90), standard_name='latitude', units='degrees')\n",
    "longitude = iris.coords.DimCoord(np.arange(0, 360,90), standard_name='longitude', units='degrees')\n",
    "cube = iris.cube.Cube(np.zeros((2, 4), np.float32), dim_coords_and_dims=[(latitude, 0), (longitude, 1)])\n",
    "cube.coord('latitude').guess_bounds()\n",
    "cube.coord('longitude').guess_bounds()\n",
    "sample = [2, 5, 4, 23], [4, 5, 15, 1]\n",
    "cube.data = sample\n",
    "median_cube = cube.collapsed(['longitude', 'latitude'], iris.analysis.MEDIAN)\n",
    "median = median_cube.data\n",
    "print(f'Median = {median}')\n",
    "\n",
    "\n",
    "'''Weighted Mean'''\n",
    "# This example is using the area of the grid cell as the weight\n",
    "import iris.analysis.cartography\n",
    "\n",
    "latitude = iris.coords.DimCoord(np.arange(-90, 90,90), standard_name='latitude', units='degrees')\n",
    "longitude = iris.coords.DimCoord(np.arange(0, 360,90), standard_name='longitude', units='degrees')\n",
    "cube = iris.cube.Cube(np.zeros((2, 4), np.float32), dim_coords_and_dims=[(latitude, 0), (longitude, 1)])\n",
    "cube.coord('latitude').guess_bounds()\n",
    "cube.coord('longitude').guess_bounds()\n",
    "sample = [2, 5, 4, 23], [4, 5, 15, 1]\n",
    "cube.data = sample\n",
    "grid_areas = iris.analysis.cartography.area_weights(cube)\n",
    "mean_cube = cube.collapsed(['longitude', 'latitude'], iris.analysis.MEAN, weights=grid_areas)\n",
    "mean = mean_cube.data\n",
    "print(f'Weighted Mean = {mean}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d0530d",
   "metadata": {},
   "source": [
    "## Exercise 1<a class=\"anchor\" id=\"exercise_1\"></a>\n",
    "\n",
    "Using any Python method, calculate the mean, median and mode for the following dataset containing a timeseries of the number of raindays per summer:\n",
    "\n",
    "1, 1, 0, 1, 2, 2, 0, 0, 0, 3, 3, 0, 3, 3, 0, 2, 2, 2, 1, 1, 4, 1, 1, 0, 3, 0, 0, 0, 1, 1, 2, 2, 2, 2, 1, 1, 1, \n",
    "1, 4, 4, 4, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 3, 3, 0, 3, 3, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 3, \n",
    "3, 3, 2, 3, 3, 1, 1, 1, 2, 2, 2, 4, 5, 5, 4, 4, 1, 1, 1, 4, 1, 1, 1, 3, 3, 5, 3, 3, 3, 2, 3, 3, 0, 0, 0, 0, 3, \n",
    "3, 3, 3, 3, 3, 0, 2, 2, 2, 2, 1, 1, 1, 3, 1, 0, 0, 0, 1, 1, 3, 1, 1, 1, 2, 2, 2, 4, 2, 2, 2, 1, 1, 1, 1, 0, 0, \n",
    "2, 2, 3, 3, 2, 2, 3, 2, 0, 0, 1, 1, 3, 3, 3, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 0, 1, 1, 1, 3, 1, 1, 1, 2, \n",
    "2, 2, 1, 1, 1, 2, 1, 1, 1, 3, 3, 5, 3, 3, 1, 1, 1, 3, 3, 3, 3, 1, 1, 1, 4, 1, 1, 4, 4, 4, 4, 4, 4, 1, 1, 1, 2,\n",
    "2, 5, 5, 2, 3, 3, 4, 4, 3, 2, 2, 2, 1, 5, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 0, 1, 1, 1, 3, 3, 3, 3, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ea754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8daa2be",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**Solution**\n",
    "\n",
    "<font color='red'>**NOTE**</font>: Your methods can include any Python library\n",
    "\n",
    "Mean = 1.84,\n",
    "Median = 2,\n",
    "Mode = 1,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f8bc5f",
   "metadata": {},
   "source": [
    "## Variability<a class=\"anchor\" id=\"variability\"></a>\n",
    "Measures of variability quantify the amount of spread in a dataset.\n",
    "\n",
    "The sample **variance:** is a measure showing how far the dataset elements are from the mean. Note that two datasets with a small and large variance, respectively, can have the same mean and median even though they can look quite different.\n",
    "\n",
    "The sample **standard deviation** is another measure of showing how far the dataset elements are from the mean, but unlike the variance, the standard deviation has the same units as the dataset elements, making it more intuitive.\n",
    "\n",
    "The sample **skewness** is a measure of the asymmetry in the dataset. Negative skewness values indicate that the dataset is skewed (has a longer tail) to the left of the mean and positive skewness values indicate that the dataset is skewed to the right of the mean. If the skewness value is close to 0, the dataset is considered to be symmetrical.\n",
    "\n",
    "The sample **kurtosis** is a measure of tailedness; how much of the data lies in the tails of the distribution, which indicates how often extreme values occur.\n",
    "\n",
    "The 10th **percentile** is the value in the dataset below which 10% of the data falls, the 20th percentile is the value in the dataset below which 20% of the data falls. The median is actually the 50th percentile because it is in the middle and so 50% of the data is below it. \n",
    "\n",
    "The difference between the 25th and 75th percentile is called the **interquartile range**, which is a measure of the dispersion in the data and is often used to identify outliers (elements in the data that fall below the 25th percentile minus 1.5 times the interquartile range or above the 75th percentile plus 1.5 times the interquartile range).\n",
    "\n",
    "The measures of variability can all be calculated using many Python libraries, including the built-in library `statistics`, and the libraries `NumPy`, `Pandas` and `Iris`, depending on the type of data being analysed. Note that for calculating the skewness for some datatypes, a dedicated statistical library will need to be used, such as `SciPy`.\n",
    "\n",
    "Note that you will need to be careful which method is choosen if there are NaNs in the dataset because, for some libraries, NaN will be returned if the sample contains NaNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfc82a8",
   "metadata": {},
   "source": [
    "### Examples using the statistics library ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8a48a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import scipy.stats\n",
    "\n",
    "sample = [2, 5, 4, 23, 4, 5, 15, 1]\n",
    "\n",
    "'''Variance'''\n",
    "variance = statistics.variance(sample)\n",
    "print(f'Variance = {variance}')\n",
    "\n",
    "'''Standard Deviation'''\n",
    "stdev = statistics.stdev(sample)\n",
    "print(f'Standard Deviation = {stdev}')\n",
    "\n",
    "'''Skewness'''\n",
    "skewness = scipy.stats.skew(sample, bias=False)\n",
    "print(f'Skewness = {skewness}')\n",
    "\n",
    "'''Kurtosis'''\n",
    "kurtosis = scipy.stats.kurtosis(sample, bias=False)\n",
    "print(f'Kurtosis = {kurtosis}')\n",
    "\n",
    "'''Percentiles'''\n",
    "percentile50th = statistics.quantiles(sample, n=2) # Note this is the same as the median\n",
    "print(f'The 50th percentile = {percentile50th}')\n",
    "percentile25th = statistics.quantiles(sample, n=4, method='inclusive')[0]\n",
    "print(f'The 25th percentile = {percentile25th}')\n",
    "percentile75th = statistics.quantiles(sample, n=4, method='inclusive')[-1]\n",
    "print(f'The 75th percentile = {percentile75th}')\n",
    "\n",
    "quartiles = statistics.quantiles(sample, n=4, method='inclusive')\n",
    "print(f'The quartiles = {quartiles}')\n",
    "\n",
    "'''Interquartile range'''\n",
    "iqr = percentile75th - percentile25th\n",
    "print(f'The interquartile range = {iqr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d877bab",
   "metadata": {},
   "source": [
    "### Examples using the NumPy library ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf782cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "# NOTE it is very important to set the number of degrees of freedom to 1 because the calculation \n",
    "# uses the mean as one of the parameters in an intermediate step.\n",
    "\n",
    "sample = [2, 5, 4, 23, 4, 5, 15, 1]\n",
    "\n",
    "'''Variance'''\n",
    "variance = np.var(sample, ddof=1)\n",
    "print(f'Variance = {variance}')\n",
    "\n",
    "# if the sample is a NumPy array you can also use:\n",
    "sample = np.array(sample)\n",
    "variance = sample.var(ddof=1)\n",
    "print(f'Variance using arrays = {variance}')\n",
    "\n",
    "# To ignore NaNs use np.nanvar()\n",
    "\n",
    "\n",
    "'''Standard Deviation'''\n",
    "stdev = sample.std(ddof=1)\n",
    "print(f'Standard Deviation = {stdev}')\n",
    "\n",
    "'''Skewness'''\n",
    "skewness = scipy.stats.skew(sample, bias=False)\n",
    "print(f'Skewness = {skewness}')\n",
    "\n",
    "'''Kurtosis'''\n",
    "kurtosis = scipy.stats.kurtosis(sample, bias=False)\n",
    "print(f'Kurtosis = {kurtosis}')\n",
    "\n",
    "'''Percentiles'''\n",
    "percentile50th = np.percentile(sample, 50) # Note this is the same as the median\n",
    "print(f'The 50th percentile = {percentile50th}')\n",
    "percentile25th = np.percentile(sample, 25)\n",
    "print(f'The 25th percentile = {percentile25th}')\n",
    "percentile75th = np.percentile(sample, 75)\n",
    "print(f'The 75th percentile = {percentile75th}')\n",
    "\n",
    "# or\n",
    "\n",
    "percentile25th = np.quantile(sample, 0.25)\n",
    "print(f'The 25th percentile = {percentile25th}')\n",
    "percentile75th = np.quantile(sample, 0.75)\n",
    "print(f'The 75th percentile = {percentile75th}')\n",
    "\n",
    "# To ignore NaNs use np.nanpercentile()\n",
    "\n",
    "quartiles = np.quantile(sample, [0.25, 0.5, 0.75])\n",
    "print(f'The quartiles = {quartiles}')\n",
    "\n",
    "'''Interquartile range'''\n",
    "iqr = percentile75th - percentile25th\n",
    "print(f'The interquartile range = {iqr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71be042a",
   "metadata": {},
   "source": [
    "### Examples using the Pandas library ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62d13b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample = [2, 5, 4, 23, 4, 5, 15, 1]\n",
    "sample_df = pd.DataFrame(sample)\n",
    "\n",
    "'''Variance'''\n",
    "variance = sample_df.var().values\n",
    "print(f'Variance = {variance}')\n",
    "\n",
    "'''Standard Deviation'''\n",
    "stdev = sample_df.std().values\n",
    "print(f'Standard Deviation = {stdev}')\n",
    "\n",
    "'''Skewness'''\n",
    "skewness = sample_df.skew().values\n",
    "print(f'Skewness = {skewness}')\n",
    "\n",
    "'''Kurtosis'''\n",
    "kurtosis = sample_df.kurtosis().values\n",
    "print(f'Kurtosis = {kurtosis}')\n",
    "\n",
    "'''Percentiles'''\n",
    "percentile50th = sample_df.quantile(0.5).values # Note this is the same as the median\n",
    "print(f'The 50th percentile = {percentile50th}')\n",
    "percentile25th = sample_df.quantile(0.25).values\n",
    "print(f'The 25th percentile = {percentile25th}')\n",
    "percentile75th = sample_df.quantile(0.75).values\n",
    "print(f'The 75th percentile = {percentile75th}')\n",
    "quartiles = sample_df.quantile([0.25, 0.5, 0.75]) # in this case a new series is returned\n",
    "print(f'The quartiles = {quartiles}')\n",
    "\n",
    "'''Interquartile range'''\n",
    "iqr = percentile75th - percentile25th\n",
    "print(f'The interquartile range = {iqr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701bc81f",
   "metadata": {},
   "source": [
    "### Examples using the Iris library ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a244210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris\n",
    "import iris.coords\n",
    "import iris.cube\n",
    "import scipy.stats\n",
    "\n",
    "latitude = iris.coords.DimCoord(np.arange(-90, 90,90), standard_name='latitude', units='degrees')\n",
    "longitude = iris.coords.DimCoord(np.arange(0, 360,90), standard_name='longitude', units='degrees')\n",
    "cube = iris.cube.Cube(np.zeros((2, 4), np.float32), dim_coords_and_dims=[(latitude, 0), (longitude, 1)])\n",
    "cube.coord('latitude').guess_bounds()\n",
    "cube.coord('longitude').guess_bounds()\n",
    "sample = [2, 5, 4, 23], [4, 5, 15, 1]\n",
    "cube.data = sample\n",
    "\n",
    "'''Variance'''\n",
    "variance = cube.data.var(ddof=1)\n",
    "print(f'Variance = {variance}')\n",
    "\n",
    "'''Standard Deviation'''\n",
    "# Using this method, we must set the number of degrees of freedom to 1.\n",
    "stdev = cube.data.std(ddof=1)\n",
    "print(f'Standard Deviation = {stdev}')\n",
    "\n",
    "# Using this method, the number of degrees of freedom is 1 by default.\n",
    "stdev_cube = cube.collapsed(['longitude', 'latitude'], iris.analysis.STD_DEV)\n",
    "stdev = stdev_cube.data\n",
    "print(f'Standard Deviation = {stdev}')\n",
    "\n",
    "'''Skewness'''\n",
    "sample = cube.data.ravel() # We must first flatten the data in the cube\n",
    "skewness = scipy.stats.skew(sample, bias=False)\n",
    "print(f'Skewness = {skewness}')\n",
    "\n",
    "'''Kurtosis'''\n",
    "sample = cube.data.ravel() # We must first flatten the data in the cube\n",
    "kurtosis = scipy.stats.kurtosis(sample, bias=False)\n",
    "print(f'Kurtosis = {kurtosis}')\n",
    "\n",
    "'''Percentiles'''\n",
    "percentile50th = np.quantile(cube.data, 0.25) # Note this is the same as the median\n",
    "print(f'The 50th percentile = {percentile50th}')\n",
    "percentile25th = np.percentile(cube.data, 25)\n",
    "print(f'The 25th percentile = {percentile25th}')\n",
    "percentile75th = np.percentile(cube.data, 75)\n",
    "print(f'The 75th percentile = {percentile75th}')\n",
    "\n",
    "quartiles = np.quantile(cube.data, [0.25, 0.5, 0.75])\n",
    "print(f'The quartiles = {quartiles}')\n",
    "\n",
    "'''Interquartile range'''\n",
    "iqr = percentile75th - percentile25th\n",
    "print(f'The interquartile range = {iqr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db497b36",
   "metadata": {},
   "source": [
    "**SciPy and Pandas can also be used to get a quick descrition of the statistics in a dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7300f4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import pandas as pd\n",
    "\n",
    "print('Using SciPy:')\n",
    "sample = [2, 5, 4, 23, 4, 5, 15, 1]\n",
    "description = scipy.stats.describe(sample, bias=False)\n",
    "print(f'{description}\\n')\n",
    "\n",
    "print('Using Pandas:')\n",
    "sample_df = pd.DataFrame(sample)\n",
    "description = sample_df.describe()\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aeff8e",
   "metadata": {},
   "source": [
    "## Exercise 2<a class=\"anchor\" id=\"exercise_2\"></a>\n",
    "\n",
    "Using any Python method, identify the outliers for the following dataset containing a timeseries of the number of raindays in July per year:\n",
    "\n",
    "1, 9, 0, 1, 2, 2, 8, 0, 0, 3, 3, 0, 3, 3, 0, 2, 2, 2, 8, 15, 4, 8, 1, 0, 3, 0, 18, 0, 12, 9, 6, 4, 2, 6, 9, 1, 1, \n",
    "9, 4, 4, 4, 1, 9, 1, 7, 2, 8, 2, 8, 2, 2, 12, 2, 1, 12, 5, 8, 1, 3, 3, 0, 3, 3, 1, 17, 6, 1, 0, 0, 1, 1, 8, 1, 15, \n",
    "3, 13, 2, 3, 3, 11, 4, 1, 9, 2, 2, 4, 5, 5, 4, 4, 1, 8, 1, 4, 1, 9, 1, 3, 3, 5, 3, 3, 3, 2, 3, 3, 0, 10, 0, 0, 13, \n",
    "3, 8, 15, 3, 3, 0, 2, 2, 2, 2, 9, 12, 1, 3, 8, 0, 0, 0, 1, 1, 3, 13, 1, 7, 2, 2, 2, 4, 2, 2, 2, 1, 8, 1, 1, 0, 10, \n",
    "8, 2, 9, 3, 2, 8, 3, 2, 0, 0, 1, 8, 8, 3, 3, 1, 9, 1, 5, 1, 2, 9, 2, 2, 1, 1, 1, 8, 0, 1, 9, 1, 3, 1, 1, 8, 2, \n",
    "2, 12, 9, 11, 9, 2, 1, 9, 1, 3, 3, 5, 3, 3, 14, 5, 1, 3, 3, 3, 3, 1, 1, 1, 4, 1, 1, 4, 4, 4, 4, 4, 4, 1, 8, 1, 2,\n",
    "2, 5, 5, 2, 3, 3, 4, 4, 3, 2, 12, 2, 1, 5, 1, 20, 2, 1, 1, 1, 2, 2, 2, 2, 2, 8, 1, 0, 1, 1, 1, 3, 3, 3, 3, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce89ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8001094",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**Solution**\n",
    "\n",
    "<font color='red'>**NOTE**</font>: Your methods can include any Python library\n",
    "\n",
    "Outliers = [12, 13, 14, 15, 17, 18, 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580ec122",
   "metadata": {},
   "source": [
    "## Visualisation<a class=\"anchor\" id=\"visualisation\"></a>\n",
    "\n",
    "We can use matplotlib to visualise the data in a statistical way by creating a histogram or a boxplot to show the distribution of the elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce30a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = [1, 9, 0, 1, 2, 2, 8, 0, 0, 3, 3, 0, 3, 3, 0, 2, 2, 2, 8, 15, 4, 8, 1, 0, 3, 0, 18, 0, 12, 9, 6, 4, 2, 6, 9, 1, 1, \n",
    "9, 4, 4, 4, 1, 9, 1, 7, 2, 8, 2, 8, 2, 2, 12, 2, 1, 12, 5, 8, 1, 3, 3, 0, 3, 3, 1, 17, 6, 1, 0, 0, 1, 1, 8, 1, 15, \n",
    "3, 13, 2, 3, 3, 11, 4, 1, 9, 2, 2, 4, 5, 5, 4, 4, 1, 8, 1, 4, 1, 9, 1, 3, 3, 5, 3, 3, 3, 2, 3, 3, 0, 10, 0, 0, 13, \n",
    "3, 8, 15, 3, 3, 0, 2, 2, 2, 2, 9, 12, 1, 3, 8, 0, 0, 0, 1, 1, 3, 13, 1, 7, 2, 2, 2, 4, 2, 2, 2, 1, 8, 1, 1, 0, 10, \n",
    "8, 2, 9, 3, 2, 8, 3, 2, 0, 0, 1, 8, 8, 3, 3, 1, 9, 1, 5, 1, 2, 9, 2, 2, 1, 1, 1, 8, 0, 1, 9, 1, 3, 1, 1, 8, 2, \n",
    "2, 12, 9, 11, 9, 2, 1, 9, 1, 3, 3, 5, 3, 3, 14, 5, 1, 3, 3, 3, 3, 1, 1, 1, 4, 1, 1, 4, 4, 4, 4, 4, 4, 1, 8, 1, 2,\n",
    "2, 5, 5, 2, 3, 3, 4, 4, 3, 2, 12, 2, 1, 5, 1, 20, 2, 1, 1, 1, 2, 2, 2, 2, 2, 8, 1, 0, 1, 1, 1, 3, 3, 3, 3, 3]\n",
    "\n",
    "plt.hist(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8761c351",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.boxplot(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Jaspy",
   "language": "python",
   "name": "jaspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
